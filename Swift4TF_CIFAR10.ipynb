{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Swift4TF_CIFAR10.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "swift",
      "display_name": "Swift"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zaidalyafeai/Notebooks/blob/master/Swift4TF_CIFAR10.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Rw1ZBHPxdsX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import Python\n",
        "import TensorFlow"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J2RItu06_wQJ",
        "colab_type": "text"
      },
      "source": [
        "Import some python libraries that we need "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SslxOdhg3Num",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%include \"EnableIPythonDisplay.swift\"\n",
        "IPythonDisplay.shell.enable_matplotlib(\"inline\")\n",
        "\n",
        "let plt = Python.import(\"matplotlib.pyplot\")\n",
        "let np  = Python.import(\"numpy\")\n",
        "let subprocess = Python.import(\"subprocess\")\n",
        "let path = Python.import(\"os.path\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "02PhB9dg_09o",
        "colab_type": "text"
      },
      "source": [
        "Download cifar 10 "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sGzbZdJH3F9B",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "ae7aa258-70c2-4b80-df3e-c27c7429c4a5"
      },
      "source": [
        "//https://github.com/tensorflow/swift-models/tree/master/CIFAR\n",
        "\n",
        "let filepath = \"./cifar-10-batches-py\"\n",
        "let isdir = Bool(path.isdir(filepath))!\n",
        "if !isdir {\n",
        "    print(\"Downloading CIFAR data...\")\n",
        "    let command = \"wget -nv -O- https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz | tar xzf - -C .\"\n",
        "    subprocess.call(command, shell: true)\n",
        "}"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading CIFAR data...\n",
            "2019-05-05 22:15:06 URL:https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz [170498071/170498071] -> \"-\" [1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C1iYm9INCh2A",
        "colab_type": "text"
      },
      "source": [
        "Setup the dataset "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1aiJm7Ht1S9c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "//https://github.com/tensorflow/swift-models/tree/master/CIFAR\n",
        "\n",
        "var batchSize:Int = 64 \n",
        "\n",
        "func loadCIFARFile(named name: String, in directory: String = \".\") -> (Tensor<Float>, Tensor<Int32>) {\n",
        "    let np = Python.import(\"numpy\")\n",
        "    let pickle = Python.import(\"pickle\")\n",
        "    let path = \"\\(directory)/cifar-10-batches-py/\\(name)\"\n",
        "    let f = Python.open(path, \"rb\")\n",
        "    let res = pickle.load(f, encoding: \"bytes\")\n",
        "\n",
        "    let bytes = res[Python.bytes(\"data\", encoding: \"utf8\")]\n",
        "    let labels = res[Python.bytes(\"labels\", encoding: \"utf8\")]\n",
        "\n",
        "    let labelTensor = Tensor<Int64>(numpy: np.array(labels))!\n",
        "    let images = Tensor<UInt8>(numpy: bytes)!\n",
        "    let imageCount = images.shape[0]\n",
        "\n",
        "    // reshape and transpose from the provided N(CHW) to TF default NHWC\n",
        "    let imageTensor = Tensor<Float>(images\n",
        "        .reshaped(to: [imageCount, 3, 32, 32])\n",
        "        .transposed(withPermutations: [0, 2, 3, 1]))\n",
        "\n",
        "    let mean = Tensor<Float>([0.485, 0.456, 0.406])\n",
        "    let std  = Tensor<Float>([0.229, 0.224, 0.225])\n",
        "    let imagesNormalized = ((imageTensor / 255.0) - mean) / std\n",
        "\n",
        "    return (imagesNormalized, Tensor<Int32>(labelTensor))\n",
        "}\n",
        "\n",
        "/// helper functions \n",
        "\n",
        "// report accuracy of a batch \n",
        "func getAccuracy(y:Tensor<Int32>, logits:Tensor<Float>) -> Float{\n",
        "  let out  = Tensor<Int32>(logits.argmax(squeezingAxis: 1) .== y).sum().scalarized()\n",
        "  return Float(out) / Float(y.shape[0])\n",
        "}\n",
        "\n",
        "//round two decimal places \n",
        "func roundTwo(_ input:Float) -> Float{\n",
        "  return (input*100).rounded()/100\n",
        "}\n",
        "\n",
        "//crop to a certain size \n",
        "func crop(_ tensor:Tensor<Float>, _ size:Int) -> Tensor<Float> {\n",
        "  let i = Int.random(in: 0..<32-size)\n",
        "  let j = Int.random(in: 0..<32-size)\n",
        "  let N = Int(tensor.shape[0])\n",
        "  \n",
        "  return tensor[0..<N, i..<i+size, j..<j+size,0..<3]\n",
        "}\n",
        "\n",
        "//randomly augment a batch \n",
        "func augment(_ tensor:Tensor<Float>) -> Tensor<Float> {\n",
        "  var out = tensor\n",
        "  \n",
        "  //maybe flip\n",
        "  if Float.random(in:0...1) < 0.5{\n",
        "    out = tensor.transposed(withPermutations: [0, 1, 2, 3])\n",
        "  }\n",
        "  //maybe crop and resize \n",
        "  if Float.random(in:0...1) < 0.5{\n",
        "    let cropped = crop(tensor, 25)\n",
        "    out = Raw.resizeArea(images:cropped , size:[32, 32] )\n",
        "  }\n",
        "  \n",
        "  return out\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RQCTZq_VC90x",
        "colab_type": "text"
      },
      "source": [
        "Create a dataset "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h9nh7E_i7A9C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "struct Element: TensorGroup {\n",
        "    var x: Tensor<Float>\n",
        "    var y: Tensor<Int32>\n",
        "}\n",
        "\n",
        "//cifar 10 training comes in 6 files we load/concatenate them [5000, 32, 32, 3]\n",
        "let train_data = (1..<6).map { loadCIFARFile(named: \"data_batch_\\($0)\") }\n",
        "\n",
        "let trainX = Raw.concat(concatDim: Tensor<Int32>(0), train_data.map { $0.0})\n",
        "let trainY = Raw.concat(concatDim: Tensor<Int32>(0), train_data.map { $0.1})\n",
        "\n",
        "//load testing images size [1000, 32, 32, 3]\n",
        "let (testX, testY) = loadCIFARFile(named: \"test_batch\")\n",
        "\n",
        "//create a dataset for training and testing \n",
        "let trainDataset = Dataset<Element>(elements: Element(x: trainX, y:trainY))\n",
        "let testDataset = Dataset<Element>(elements:  Element(x:testX, y:testY))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8f_A1GMKDmsW",
        "colab_type": "text"
      },
      "source": [
        "Create the basic parts of the mode [convblocks + classifier]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EiFHwXwiGohF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "struct ConvBlock:Layer{\n",
        "\n",
        "  typealias Input = Tensor<Float>\n",
        "  typealias Output = Tensor<Float>\n",
        "  \n",
        "  var conv1: Conv2D<Float>\n",
        "  var conv2: Conv2D<Float>\n",
        "  var pool: MaxPool2D<Float>\n",
        "  var norm: BatchNorm<Float>\n",
        "  \n",
        "  init(filterShape:(Int, Int))\n",
        "  {\n",
        "    self.conv1 = Conv2D<Float>(filterShape: (3, 3,filterShape.0, filterShape.1), \n",
        "                              strides: (1, 1), padding : .same, activation: relu)\n",
        "    \n",
        "    self.conv2 = Conv2D<Float>(filterShape: (3, 3,filterShape.1, filterShape.1), \n",
        "                              strides: (1, 1), padding : .same, activation: relu)\n",
        "    \n",
        "    self.norm = BatchNorm<Float>(featureCount: filterShape.1)\n",
        "    self.pool = MaxPool2D<Float>(poolSize: (2, 2), strides: (2, 2))\n",
        "  }\n",
        "  \n",
        "  @differentiable\n",
        "  func call(_ input: Input) -> Output {\n",
        "    return input.sequenced(through: conv1, conv2, norm, pool)\n",
        "  }\n",
        "}\n",
        "\n",
        "struct Classifier:Layer{\n",
        "\n",
        "  typealias Input = Tensor<Float>\n",
        "  typealias Output = Tensor<Float>\n",
        "  \n",
        "  var dense1: Dense<Float>\n",
        "  var dense2: Dense<Float>\n",
        "  var dropout: Dropout<Float>\n",
        "  \n",
        "  init(input:Int, mid:Int)\n",
        "  {\n",
        "    self.dense1 = Dense<Float>(inputSize: input , outputSize: mid, activation: relu)\n",
        "    self.dropout = Dropout<Float>(probability: 0.5)\n",
        "    self.dense2 = Dense<Float>(inputSize: mid , outputSize: 10)\n",
        "  }\n",
        "  \n",
        "  @differentiable\n",
        "  func call(_ input: Input) -> Output {\n",
        "    return input.sequenced(through: dense1, dropout, dense2)  \n",
        "  }\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g8JIXXijD2kV",
        "colab_type": "text"
      },
      "source": [
        "Create the overall model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3cwhwVuJcenw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "struct CNN: Layer {\n",
        "    typealias Input = Tensor<Float>\n",
        "    typealias Output = Tensor<Float>\n",
        "\n",
        "    var conv1 = ConvBlock(filterShape:(3, 16))\n",
        "    var conv2 = ConvBlock(filterShape:(16, 32))\n",
        "    var conv3 = ConvBlock(filterShape:(32, 64))\n",
        "    var conv4 = ConvBlock(filterShape:(64, 64))\n",
        "  \n",
        "    var dropout = Dropout<Float>(probability: 0.5)\n",
        "  \n",
        "    var flatten = Flatten<Float>()\n",
        "    var classifier = Classifier(input:2*2*64, mid:128)\n",
        "    \n",
        "    @differentiable\n",
        "    func call(_ input: Input) -> Output {\n",
        "        let convolved = input.sequenced(through: conv1, conv2, conv3, conv4)\n",
        "        return convolved.sequenced(through:dropout, flatten, classifier)\n",
        "    }\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iqnb1PSJavPO",
        "colab_type": "code",
        "outputId": "3c95e9ea-5f1a-4daa-9af7-66e99f19afb5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "var model = CNN()\n",
        "let optimizer = Adam(for: model)\n",
        "\n",
        "//warmup \n",
        "let tensor = Tensor<Float>(zeros: [1, 32, 32, 3])\n",
        "print(model(tensor))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mEBAvChZD7DS",
        "colab_type": "text"
      },
      "source": [
        "Training and reporting the results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FSF0UUasAXVN",
        "colab_type": "code",
        "outputId": "612d10c9-97b8-4d55-a2de-23a3a54a5302",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 926
        }
      },
      "source": [
        "var trainLoss:Float = 0.0\n",
        "var trainAcc :Float = 0.0\n",
        "var testLoss:Float = 0.0\n",
        "var testAcc:Float = 0.0 \n",
        "\n",
        "var batchCount: Float = 0.0\n",
        "\n",
        "for epoch in 0..<50{\n",
        "  \n",
        "  //evaluate metrics\n",
        "  trainLoss = 0.0\n",
        "  trainAcc  = 0.0\n",
        "  batchCount = 0.0 \n",
        "  \n",
        "  let shuffled = trainDataset.shuffled(sampleCount:50000 , randomSeed: Int64(epoch))\n",
        "    \n",
        "  for batch in shuffled.batched(batchSize) {\n",
        "  \n",
        "    //get batches\n",
        "    let X = augment(batch.x)\n",
        "    let y = batch.y\n",
        "    \n",
        "    //calculate the loss and gradient\n",
        "    let (loss, grads) = valueWithGradient(at: model) { model -> Tensor<Float> in\n",
        "            let logits = model(X)\n",
        "            return softmaxCrossEntropy(logits: logits, labels: y)\n",
        "    }\n",
        "\n",
        "    //make an optimizer step \n",
        "    optimizer.update(&model.allDifferentiableVariables, along: grads)    \n",
        "    \n",
        "    let logits = model(X) //this is slowing down ? \n",
        "    let acc = getAccuracy(y:y, logits:logits)\n",
        "    \n",
        "    trainLoss += Float(loss.scalarized())\n",
        "    trainAcc  += acc\n",
        "    batchCount += 1\n",
        "  }  \n",
        " \n",
        "  trainLoss /= batchCount\n",
        "  trainAcc  /= batchCount\n",
        " \n",
        "  //training\n",
        "  testLoss = 0.0\n",
        "  testAcc  = 0.0\n",
        "  \n",
        "  let logits = model(testX)\n",
        "  let loss = softmaxCrossEntropy(logits: logits, labels: testY)\n",
        "  let acc = getAccuracy(y:testY, logits:logits)\n",
        "\n",
        "  testLoss += Float(loss.scalarized())\n",
        "  testAcc  += acc\n",
        "  print(\"epoch: \\(epoch+1), train_loss: \\(roundTwo(trainLoss)), test_loss: \\(roundTwo(testLoss)), train_acc: \\(roundTwo(trainAcc)), test_acc: \\(roundTwo(testAcc))\" )\n",
        "\n",
        "}"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch: 1, train_loss: 1.64, test_loss: 1.3, train_acc: 0.41, test_acc: 0.53\n",
            "epoch: 2, train_loss: 1.23, test_loss: 1.1, train_acc: 0.58, test_acc: 0.61\n",
            "epoch: 3, train_loss: 1.04, test_loss: 0.94, train_acc: 0.66, test_acc: 0.67\n",
            "epoch: 4, train_loss: 0.92, test_loss: 0.83, train_acc: 0.7, test_acc: 0.71\n",
            "epoch: 5, train_loss: 0.84, test_loss: 0.87, train_acc: 0.73, test_acc: 0.7\n",
            "epoch: 6, train_loss: 0.78, test_loss: 0.76, train_acc: 0.76, test_acc: 0.74\n",
            "epoch: 7, train_loss: 0.74, test_loss: 0.76, train_acc: 0.77, test_acc: 0.74\n",
            "epoch: 8, train_loss: 0.69, test_loss: 0.77, train_acc: 0.79, test_acc: 0.73\n",
            "epoch: 9, train_loss: 0.66, test_loss: 0.72, train_acc: 0.8, test_acc: 0.75\n",
            "epoch: 10, train_loss: 0.63, test_loss: 0.69, train_acc: 0.81, test_acc: 0.77\n",
            "epoch: 11, train_loss: 0.61, test_loss: 0.67, train_acc: 0.82, test_acc: 0.77\n",
            "epoch: 12, train_loss: 0.59, test_loss: 0.7, train_acc: 0.82, test_acc: 0.76\n",
            "epoch: 13, train_loss: 0.58, test_loss: 0.71, train_acc: 0.83, test_acc: 0.77\n",
            "epoch: 14, train_loss: 0.56, test_loss: 0.69, train_acc: 0.84, test_acc: 0.77\n",
            "epoch: 15, train_loss: 0.55, test_loss: 0.71, train_acc: 0.84, test_acc: 0.77\n",
            "epoch: 16, train_loss: 0.52, test_loss: 0.69, train_acc: 0.85, test_acc: 0.78\n",
            "epoch: 17, train_loss: 0.51, test_loss: 0.68, train_acc: 0.86, test_acc: 0.78\n",
            "epoch: 18, train_loss: 0.51, test_loss: 0.66, train_acc: 0.85, test_acc: 0.78\n",
            "epoch: 19, train_loss: 0.5, test_loss: 0.7, train_acc: 0.86, test_acc: 0.77\n",
            "epoch: 20, train_loss: 0.5, test_loss: 0.7, train_acc: 0.86, test_acc: 0.78\n",
            "epoch: 21, train_loss: 0.48, test_loss: 0.66, train_acc: 0.86, test_acc: 0.79\n",
            "epoch: 22, train_loss: 0.48, test_loss: 0.7, train_acc: 0.86, test_acc: 0.78\n",
            "epoch: 23, train_loss: 0.46, test_loss: 0.67, train_acc: 0.87, test_acc: 0.79\n",
            "epoch: 24, train_loss: 0.45, test_loss: 0.69, train_acc: 0.87, test_acc: 0.79\n",
            "epoch: 25, train_loss: 0.44, test_loss: 0.72, train_acc: 0.88, test_acc: 0.78\n",
            "epoch: 26, train_loss: 0.44, test_loss: 0.69, train_acc: 0.88, test_acc: 0.78\n",
            "epoch: 27, train_loss: 0.43, test_loss: 0.72, train_acc: 0.88, test_acc: 0.78\n",
            "epoch: 28, train_loss: 0.42, test_loss: 0.68, train_acc: 0.88, test_acc: 0.79\n",
            "epoch: 29, train_loss: 0.43, test_loss: 0.67, train_acc: 0.88, test_acc: 0.8\n",
            "epoch: 30, train_loss: 0.42, test_loss: 0.71, train_acc: 0.89, test_acc: 0.78\n",
            "epoch: 31, train_loss: 0.4, test_loss: 0.72, train_acc: 0.89, test_acc: 0.78\n",
            "epoch: 32, train_loss: 0.41, test_loss: 0.69, train_acc: 0.89, test_acc: 0.79\n",
            "epoch: 33, train_loss: 0.39, test_loss: 0.73, train_acc: 0.9, test_acc: 0.79\n",
            "epoch: 34, train_loss: 0.39, test_loss: 0.7, train_acc: 0.9, test_acc: 0.79\n",
            "epoch: 35, train_loss: 0.38, test_loss: 0.7, train_acc: 0.9, test_acc: 0.79\n",
            "epoch: 36, train_loss: 0.38, test_loss: 0.71, train_acc: 0.9, test_acc: 0.79\n",
            "epoch: 37, train_loss: 0.37, test_loss: 0.68, train_acc: 0.9, test_acc: 0.8\n",
            "epoch: 38, train_loss: 0.37, test_loss: 0.69, train_acc: 0.9, test_acc: 0.8\n",
            "epoch: 39, train_loss: 0.37, test_loss: 0.72, train_acc: 0.9, test_acc: 0.8\n",
            "epoch: 40, train_loss: 0.36, test_loss: 0.72, train_acc: 0.91, test_acc: 0.79\n",
            "epoch: 41, train_loss: 0.34, test_loss: 0.71, train_acc: 0.91, test_acc: 0.79\n",
            "epoch: 42, train_loss: 0.35, test_loss: 0.74, train_acc: 0.91, test_acc: 0.79\n",
            "epoch: 43, train_loss: 0.35, test_loss: 0.73, train_acc: 0.91, test_acc: 0.79\n",
            "epoch: 44, train_loss: 0.35, test_loss: 0.74, train_acc: 0.91, test_acc: 0.79\n",
            "epoch: 45, train_loss: 0.34, test_loss: 0.74, train_acc: 0.91, test_acc: 0.79\n",
            "epoch: 46, train_loss: 0.34, test_loss: 0.72, train_acc: 0.91, test_acc: 0.8\n",
            "epoch: 47, train_loss: 0.34, test_loss: 0.71, train_acc: 0.91, test_acc: 0.8\n",
            "epoch: 48, train_loss: 0.32, test_loss: 0.73, train_acc: 0.92, test_acc: 0.79\n",
            "epoch: 49, train_loss: 0.34, test_loss: 0.69, train_acc: 0.91, test_acc: 0.8\n",
            "epoch: 50, train_loss: 0.32, test_loss: 0.73, train_acc: 0.92, test_acc: 0.8\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}